{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1262ce44-84a1-4d43-8f8f-ac21f9b10c11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateus/Desktop/FATEC/6-semestre/IA_Walmir/prova-pratica/envProva/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc_21\n",
      "Documento: frank meadows audiological scientist male\n",
      "Distância: 0.96626877784729\n",
      "----------------------------------------\n",
      "ID: doc_42\n",
      "Documento: melody cookr research scientist life science\n",
      "Distância: 1.2876564264297485\n",
      "----------------------------------------\n",
      "ID: doc_43\n",
      "Documento: science male\n",
      "Distância: 1.4285218715667725\n",
      "----------------------------------------\n",
      "ID: doc_106\n",
      "Documento: science male\n",
      "Distância: 1.4285218715667725\n",
      "----------------------------------------\n",
      "ID: doc_113\n",
      "Documento: melodyr cox dance movement psychotherapist\n",
      "Distância: 1.4420312643051147\n",
      "----------------------------------------\n",
      "ID: doc_78\n",
      "Documento: shari daugherty curator maler\n",
      "Distância: 1.5335760116577148\n",
      "----------------------------------------\n",
      "ID: doc_51\n",
      "Documento: Matthew stone scientist clinical\n",
      "Distância: 1.577803134918213\n",
      "----------------------------------------\n",
      "ID: doc_104\n",
      "Documento: education femalar\n",
      "Distância: 1.5895439386367798\n",
      "----------------------------------------\n",
      "ID: doc_70\n",
      "Documento: teacher femaler\n",
      "Distância: 1.6038081645965576\n",
      "----------------------------------------\n",
      "ID: doc_89\n",
      "Documento: tamar hull english second language teacher male\n",
      "Distância: 1.6168724298477173\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importações necessárias\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pdfplumber\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import chromadb\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# 1. Função para ler PDF\n",
    "def ler_pdf(caminho_pdf):\n",
    "    with pdfplumber.open(caminho_pdf) as leitor_pdf:\n",
    "        texto = \"\".join([pagina.extract_text() for pagina in leitor_pdf.pages])\n",
    "        leitor_pdf.close()\n",
    "    return texto.replace(\"\\n\", \" \")\n",
    "\n",
    "# 1.1. Função para ler CSV\n",
    "def ler_csv(caminho_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(caminho_csv)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro  ao ler CSV: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 1.2 Função para ler uma coluna de um arquivo CSV\n",
    "def processar_linhas_csv(caminho_csv, coluna_texto):\n",
    "    df = pd.read_csv(caminho_csv) # lê arquivo CSV\n",
    "    textos_tratados = []\n",
    "    \n",
    "    for texto in df[coluna_texto]:\n",
    "        texto_processado = tratamento_pln(str(texto)) # trata o texto da coluna\n",
    "        textos_tratados.append(texto_processado) # adiciona na lista de textos tratados\n",
    "    \n",
    "    return f\"\\n\".join(textos_tratados)\n",
    "\n",
    "def combinar_colunas_csv(caminho_csv, colunas):\n",
    "    df = pd.read_csv(caminho_csv).to_dict(\"records\") ## transforma uma lista de dicionários\n",
    "    texto_combinado = []\n",
    "    \n",
    "    for obj in df:\n",
    "        texto = \"\"\n",
    "        for coluna in colunas:\n",
    "            texto += f\"{obj[coluna]} \" ## faz uma linha com os valores das colunas selecionadas\n",
    "        texto_combinado.append(texto)\n",
    "        \n",
    "    return f\"\\n\".join(texto_combinado)\n",
    "\n",
    "def transformar_dataframe_lista(df : pd.DataFrame, coluna : str) :\n",
    "    return df[coluna].astype(str).tolist()\n",
    " \n",
    "# 2. Função de pré-processamento de texto\n",
    "def tratamento_pln(texto):\n",
    "    # Carregar modelo e stopwords\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    # Normalização\n",
    "    texto = texto.lower() ## deixa todas as letras minúsculas \n",
    "    texto = re.sub(r'[^a-zA-Záéíóú\\s]', '', texto) # Remoção de números, pontuações e caracteres especiais, utilizando regex \n",
    "    \n",
    "    # Tokenização e limpeza\n",
    "    doc = nlp(texto) # tokenização do texto\n",
    "    clean_tokens = [token.lemma_ for token in doc \n",
    "                   if token.text not in stop_words and not token.is_punct] # tokens lematizados e sem stop words e pontuações\n",
    "    \n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# 3. Divisão em chunks\n",
    "def criar_chunks(texto, tamanho=30, overlap=10):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=tamanho,\n",
    "        chunk_overlap=overlap\n",
    "    ) # instancia o modelo setando o tamanho dos chunks em 40 com o overlap de 10\n",
    "    return splitter.split_text(texto) # retorna os chunks \n",
    "\n",
    "# 4. Geração de embeddings e armazenamento\n",
    "def criar_banco_vetorial(chunks, nome_colecao=\"colecao_teste\"):\n",
    "    # Gerar embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') ## instancia o modelo de geração de embeddings\n",
    "    embeddings = model.encode(chunks) ## transforma os chunks em embeddings para serem armazenados no banco vetorial\n",
    "    \n",
    "    # Criar banco vetorial\n",
    "    client = chromadb.Client() ## instância o banco de dados\n",
    "    collection = client.create_collection(name=nome_colecao) ## cria a collection\n",
    "    \n",
    "    # Adicionar documentos\n",
    "    ids = [f\"doc_{i}\" for i in range(len(chunks))] ## Cria os ids dos documentos a partir da quantidade de chunks que serão armazenados\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings.tolist(),  \n",
    "        ids=ids\n",
    "    ) ## adiciona os dados vetorizados na collection\n",
    "    \n",
    "    return collection, model\n",
    "\n",
    "# 5. Função de consulta\n",
    "def consultar_banco(colecao, modelo, consulta, n_resultados=1):\n",
    "    embedding_consulta = modelo.encode([consulta]) ## transforma a consulta em um valor dado vetoriazado para realizar a busca no banco\n",
    "    resultados = colecao.query(\n",
    "        query_embeddings=embedding_consulta.tolist(),\n",
    "        n_results=n_resultados ## quantidade de resultados que serão retornados\n",
    "    )\n",
    "    return resultados\n",
    "    \n",
    "## CSV\n",
    "texto_csv = combinar_colunas_csv(\"people-100.csv\", [\"First Name\", \"Last Name\", \"Job Title\", \"Sex\"]) # extrai o dados das colunas do CSV em formato de string\n",
    "\n",
    "# print(texto_csv)\n",
    "\n",
    "## PDF\n",
    "# texto = ler_pdf(\"chapeuzinho.pdf\")\n",
    "\n",
    "# Passo 2: Pré-processar\n",
    "texto_tratado = tratamento_pln(texto_csv)\n",
    "\n",
    "# # Passo 3: Criar chunks\n",
    "chunks = criar_chunks(texto_tratado, 50)\n",
    "\n",
    "# # Passo 4: Banco vetorial\n",
    "colecao, modelo = criar_banco_vetorial(chunks, \"peoples\")\n",
    "\n",
    "# # Passo 5: Consulta de exemplo\n",
    "resultados = consultar_banco(colecao, modelo, \"Audiological\",10) \n",
    "\n",
    "# # Exibir resultados\n",
    "for i in range(len(resultados['ids'][0])):\n",
    "    print(f\"ID: {resultados['ids'][0][i]}\")\n",
    "    print(f\"Documento: {resultados['documents'][0][i]}\")\n",
    "    print(f\"Distância: {resultados['distances'][0][i]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9242c35-1637-413e-be57-877b107cb613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prova-walmir",
   "language": "python",
   "name": "prova-walmir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
